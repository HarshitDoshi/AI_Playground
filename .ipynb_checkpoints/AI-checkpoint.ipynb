{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52eVtvU8BjXE"
   },
   "source": [
    "# Harshit's Artificial Intelligence (AI) Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3kMdBCZCv05"
   },
   "source": [
    "According to me, the field of AI is roughly divided into the following sub-domains:\n",
    "\n",
    "- Data Science\n",
    "- Learning\n",
    "- Robotics\n",
    "\n",
    "### Data Science\n",
    "Data science is the field which deals with the large amount of data we require and have in the worldly problems. The following things maybe roughly put in the field of data science:\n",
    "\n",
    "- Data Storage\n",
    "- Databases\n",
    "- Data Mining\n",
    "- Data Exploration\n",
    "- Big Data\n",
    "- Data Cleansing\n",
    "- Data Analysis\n",
    "- Data Visualization And Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfunfbMbYlUH"
   },
   "source": [
    "### Learning\n",
    "\n",
    "Learning is the science in which we make computers or machines learn in a way which is quite similar to how a human learns. Learning is mostly about mathematics. Statistics plays a major role in learning and hence, in general, it is also known as \"statistical learning\". When statistical learning methods are converted to computer algorithms and programs, we call it \"machine learning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txCBDjJCYpU1"
   },
   "source": [
    "### Robotics\n",
    "\n",
    "Robotics is the science of creating machines which try to replicate how a human body works using mechanical, electrical, computer and electronic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjEy927gHl97"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9qugXqxCr4N"
   },
   "source": [
    "## Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdofZocwCMDu"
   },
   "source": [
    "There are 2 aspects of every statistical learning problems. They are:\n",
    "\n",
    "- Input Variable\n",
    "- Output Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSmq2_0CCp4G"
   },
   "source": [
    "### Input Variables\n",
    "Input variables are the various factors affecting the _output_ or the _result_ of any statistical learning problem. They are also known as _predictors_, _independent variables_ or _features_.\n",
    "\n",
    "Example:\n",
    "\n",
    "In the advertising budget problem, the budget of **television based advertisements**, **radio based advertisements** and **newspaper based advertisements** can be shown by variables, **$X_{1}$**, **$X_{2}$** and **$X_{3}$** respectively.\n",
    "\n",
    "Factors | Variables\n",
    "--- | ---\n",
    "TV | $X_{1}$\n",
    "RADIO | $X_{2}$\n",
    "NEWSPAPER | $X_{3}$\n",
    "\n",
    "These are the input variables for the advertising budget problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HPBgHCCoHfZh"
   },
   "source": [
    "### Output Variables\n",
    "\n",
    "The final result of a prediction or learning problem. It is the outcome of the whole problem. The output variables are dependent on the input variables for that particular problem. They are also known as _responses_ or _dependent variables_.\n",
    "\n",
    "Example:\n",
    "In the advertising budget problem, after we have predicted the **sales** from the given input variables, using one of the many statistical learning methods, we get the resultant **sales**. We can represent that as **$Y$**.\n",
    "\n",
    "Result | Variable\n",
    "--- | ---\n",
    "Sales | $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SzvTaVDMqEM"
   },
   "source": [
    "### Relationship between the input variables and the output variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYj6Yf3nI4ci"
   },
   "source": [
    "Thus, we observe a _quantitative response_ ($Y$), due to _$p$_ different _predictors_ ($X_{1}, X_{2}, X_{3}, \\cdots , X_{p}$).\n",
    "\n",
    "Summing up the $p$ predictors into one, we get:\n",
    "$$\n",
    "X = X_{1} + X_{2} + X_{3} + \\cdots + X_{p}\n",
    "$$\n",
    "\n",
    "Also, $X$ affects $Y$. Thus, there is some relationship between $X$ and $Y$.\n",
    "\n",
    "It can be shown by the following equation:\n",
    "$$\n",
    "Y = f(X) + \\varepsilon\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $f(X)$ is a fixed but unknown function which is dependent on $X$,\n",
    "- $\\varepsilon$ is the _error term_ which is independent of $X$ and has a mean of _zero_. Errors are **_positive_** if the observation lies **above** the _curve of $f(X)$_ and **_negative_** if they lie **below** it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sW1rFAy1MYVo"
   },
   "source": [
    "Our goal is to find an estimate of $f$, which would fit $X$ to $Y$ with the minimum error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqg7A-ZDMk7T"
   },
   "source": [
    "### Why estimate $f$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pziy3rmDM3SP"
   },
   "source": [
    "Our motive behind estimating $f$ can be one (or both) of the following:\n",
    "1. Prediction\n",
    "2. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKctojwhfRnt"
   },
   "source": [
    "### Prediction\n",
    "\n",
    "Prediction means trying to estimate a result for the future based on the past. We humans predict something by acknowledging the data from the past and trying to guess or estimate the future. Similarly, machines can be taught using various learning techniques and they can then estimate or guess the future. Examples of prediction or domains where prediction can be applied are:\n",
    "\n",
    "- Weather forecasts\n",
    "- Disaster analysis\n",
    "- Stock markets\n",
    "- Traffic forecast\n",
    "\n",
    "For prediction, we have a set of _inputs_, $X$, readily available. The _output_, $Y$, is not available to us.\n",
    "\n",
    "We can then predict $Y$ as follows:\n",
    "$$\n",
    "\\hat{Y} = \\hat{f}(X)\n",
    "$$\n",
    "\n",
    "Where,\n",
    "- $\\hat{Y}$ is our prediction for $Y$, and,\n",
    "- $\\hat{f}(X)$ is our estimate for $f(X)$.\n",
    "\n",
    "Here, the error term, $\\varepsilon$, averages to zero.\n",
    "\n",
    "The accuracy of $\\hat{Y}$, as a prediction of $Y$, depends on 2 quantities,\n",
    "1. Reducible error\n",
    "2. Irreducible error\n",
    "\n",
    "In general, $\\hat{f}$ will not be an accurate estimate for $f$, and it will introduce a _reducible error_. We can reduce or minimize it using better statistical learning methods.\n",
    "\n",
    "Even though we perfectly estimate $f$ such that $\\hat{Y} = f(X)$, our prediction would still contain an error, because, $Y$ is also a function of $\\varepsilon$. Variability associated with $\\varepsilon$ also affects the accuracy of our prediction. This is the _irreducible error_. We cannot remove it how much ever we try.\n",
    "\n",
    "**Why is the irreducible error > 0?**\n",
    "\n",
    "$\\varepsilon$ may contain _unmeasurable variables_ that are useful in predicting $Y$. It may also contain _unmeasurable variations_.\n",
    "\n",
    "$$\n",
    "E(Y - \\hat{Y})^{2} = E[f(X) + \\varepsilon - \\hat{f}(X)]^{2} = [f(X) - \\hat{f}(X)]^{2} + var(\\varepsilon)\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $E(Y - \\hat{Y})^{2}$ is the _expected value_,\n",
    "- $[f(X) - \\hat{f}(X)]^{2}$ is the _squared difference_ between the predicted and actual value of $Y$. It is _reducible_ in nature.\n",
    "- $var({\\varepsilon})$ is the variance associated with $\\varepsilon$. It is irreducible in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SES4UO2Vx2oY"
   },
   "source": [
    "### Inference\n",
    "\n",
    "We are often interested in knowing how the output of a problem is affected by the input. We want to know how $Y$ is affected as $X = X_{1} + X_{2} + X_{3} + \\cdots + X_{p}$ changes.\n",
    "\n",
    "Thus, we want to understand the relationship between $X$ and $Y$.\n",
    "\n",
    "Here, $\\hat{f}$ cannot be treated as a \"_black box_\". We need the exact form of $\\hat{f}$.\n",
    "\n",
    "**We may want to infer,**\n",
    "- which predictors are _associated_ with the response,\n",
    "- what the _relationship between each_ predictor and the response is (positive, negative, etc.).\n",
    "- can the relationship between $Y$ and each predictor be adequately summarized using a linear equation or is the relationship more complicated (quadratic, cubic, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPHTH1Zv1P1I"
   },
   "source": [
    "### How do we estimate $f$?\n",
    "\n",
    "To estimate $f$, we need to teach our method. To teach, we have some data which we call as the _training data_. Training data is the dataset or part of the dataset which is used to train or teach the method on how to estimate $f$.\n",
    "\n",
    "**Characteristics of training data:**\n",
    "\n",
    "- $i$ denotes the $i^{th}$ observation out of the total $n$ observations.\n",
    "- $j$ denotes the $j^{th}$ predictor out of the $p$ total predictors.\n",
    "\n",
    "Thus, $x_{ij}$ is the $i^{th}$ observation of the $j^{th}$ predictor.\n",
    "\n",
    "Thus, $y_{i}$ is the response variable for the $i^{th}$ observation.\n",
    "\n",
    "Thus,\n",
    "\n",
    "Our training data set consists of,\n",
    "$$\n",
    "{(x_{1}, y_{1}), (x_{2}, y_{2}, \\cdots, (x_{n}, y_{n}))}\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "x_{i} = (x_{i1}, x_{i2}, \\cdots, x_{ip})^{T}\n",
    "$$\n",
    "\n",
    "Our goal is to apply a statistical learning method to our training data in order to estimate the unknown fucntion $f$.\n",
    "\n",
    "We want to find a function $f$ such that,\n",
    "$Y \\approx \\hat{f}(X)$, for any observation $(X, Y)$.\n",
    "\n",
    "Most statistical methods for this task can be classified into:\n",
    "\n",
    "1. Parametric methods\n",
    "2. Non-parametrix methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7oWjpFFT7JL"
   },
   "source": [
    "### Parametric Methods\n",
    "\n",
    "**Involves a 2-step, _model based_ approach.**\n",
    "\n",
    "**STEP 1:**\n",
    "\n",
    "We, first, make an assumption about the functional form, or shape, of $f$.\n",
    "\n",
    "One very simple, and maybe the first assumption we may make for any given problem, would be that $f$ is _linear_ in $X$:\n",
    "\n",
    "$$\n",
    "f(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{p}X_{p}\n",
    "$$\n",
    "\n",
    "This is a _linear model_.\n",
    "\n",
    "Linear models are very simple. Instead of having to estimate an entirely arbitrary $p$-dimensional function $f(X)$, one only needs to estimate the $p + 1$ coefficients, $\\beta_{0}, \\beta_{1}, \\beta_{2}, \\cdots, \\beta_{p}$.\n",
    "\n",
    "**STEP 2:**\n",
    "\n",
    "After a model is selected, a _procedure_ needs to be selected to _fit_ or _train_ the model. In case of our linear model, we need to estimate the parameters $\\beta_{0}, \\beta_{1}, \\beta_{2}, \\cdots, \\beta_{p}$ such that,\n",
    "\n",
    "$$\n",
    "Y \\approx \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{p}X_{p}\n",
    "$$\n",
    "\n",
    "The most common approach to fitting the model is referred to as \"**_(ordinary) least squares_**\". However, _least squares_ is one of the many possible methods to fit a linear model.\n",
    "\n",
    "**Thus, a _parametric method_ reduces the problem of estimating $f$ down to one of estimating a set of parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "A *Decision Tree* is a very basic model used in machine learning. It is not very accurate for predictions and much more powerful models do exist, but they are very easy to understand and sometimes also act as building blocks for the more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a simple decision tree:\n",
    "\n",
    "![Simple Decision Tree (PNG)](/res/simple_decision_tree.svg)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
