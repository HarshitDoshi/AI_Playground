{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52eVtvU8BjXE"
   },
   "source": [
    "# Harshit's Artificial Intelligence (AI) Notes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3kMdBCZCv05"
   },
   "source": [
    "According to me, the field of AI is roughly divided into the following sub-domains:\n",
    "\n",
    "- Data Science\n",
    "- Learning\n",
    "- Robotics\n",
    "\n",
    "### Data Science\n",
    "Data science is the field which deals with the large amount of data we require and have in the worldly problems. The following things maybe roughly put in the field of data science:\n",
    "\n",
    "- Data Storage\n",
    "- Databases\n",
    "- Data Mining\n",
    "- Data Exploration\n",
    "- Big Data\n",
    "- Data Cleansing\n",
    "- Data Analysis\n",
    "- Data Visualization And Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfunfbMbYlUH"
   },
   "source": [
    "### Learning\n",
    "\n",
    "Learning is the science in which we make computers or machines learn in a way which is quite similar to how a human learns. Learning is mostly about mathematics. Statistics plays a major role in learning and hence, in general, it is also known as \"statistical learning\". When statistical learning methods are converted to computer algorithms and programs, we call it \"machine learning\".\n",
    "\n",
    "The following are the sections in which we have divided **Learning** in:\n",
    "\n",
    "* Statistical Learning\n",
    "    * Input Variables\n",
    "    * Output Variables\n",
    "    * Relationship between input variables and output variables\n",
    "    * Why estimate $f$?\n",
    "    * Prediction\n",
    "    * Inference\n",
    "    * How do we estimate $f$?\n",
    "    * Parametric methods\n",
    "* Machine Learning\n",
    "    * Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txCBDjJCYpU1"
   },
   "source": [
    "### Robotics\n",
    "\n",
    "Robotics is the science of creating machines which try to replicate how a human body works using mechanical, electrical, computer and electronic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjEy927gHl97"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9qugXqxCr4N"
   },
   "source": [
    "## Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdofZocwCMDu"
   },
   "source": [
    "There are 2 aspects of every statistical learning problems. They are:\n",
    "\n",
    "- Input Variable\n",
    "- Output Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSmq2_0CCp4G"
   },
   "source": [
    "### Input Variables\n",
    "Input variables are the various factors affecting the _output_ or the _result_ of any statistical learning problem. They are also known as _predictors_, _independent variables_ or _features_.\n",
    "\n",
    "Example:\n",
    "\n",
    "In the advertising budget problem, the budget of **television based advertisements**, **radio based advertisements** and **newspaper based advertisements** can be shown by variables, **$X_{1}$**, **$X_{2}$** and **$X_{3}$** respectively.\n",
    "\n",
    "Factors | Variables\n",
    "--- | ---\n",
    "TV | $X_{1}$\n",
    "RADIO | $X_{2}$\n",
    "NEWSPAPER | $X_{3}$\n",
    "\n",
    "These are the input variables for the advertising budget problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HPBgHCCoHfZh"
   },
   "source": [
    "### Output Variables\n",
    "\n",
    "The final result of a prediction or learning problem. It is the outcome of the whole problem. The output variables are dependent on the input variables for that particular problem. They are also known as _responses_ or _dependent variables_.\n",
    "\n",
    "Example:\n",
    "In the advertising budget problem, after we have predicted the **sales** from the given input variables, using one of the many statistical learning methods, we get the resultant **sales**. We can represent that as **$Y$**.\n",
    "\n",
    "Result | Variable\n",
    "--- | ---\n",
    "Sales | $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SzvTaVDMqEM"
   },
   "source": [
    "### Relationship between the input variables and the output variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYj6Yf3nI4ci"
   },
   "source": [
    "Thus, we observe a _quantitative response_ ($Y$), due to _$p$_ different _predictors_ ($X_{1}, X_{2}, X_{3}, \\cdots , X_{p}$).\n",
    "\n",
    "Summing up the $p$ predictors into one, we get:\n",
    "$$\n",
    "X = X_{1} + X_{2} + X_{3} + \\cdots + X_{p}\n",
    "$$\n",
    "\n",
    "Also, $X$ affects $Y$. Thus, there is some relationship between $X$ and $Y$.\n",
    "\n",
    "It can be shown by the following equation:\n",
    "$$\n",
    "Y = f(X) + \\varepsilon\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $f(X)$ is a fixed but unknown function which is dependent on $X$,\n",
    "- $\\varepsilon$ is the _error term_ which is independent of $X$ and has a mean of _zero_. Errors are **_positive_** if the observation lies **above** the _curve of $f(X)$_ and **_negative_** if they lie **below** it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sW1rFAy1MYVo"
   },
   "source": [
    "Our goal is to find an estimate of $f$, which would fit $X$ to $Y$ with the minimum error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqg7A-ZDMk7T"
   },
   "source": [
    "### Why estimate $f$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pziy3rmDM3SP"
   },
   "source": [
    "Our motive behind estimating $f$ can be one (or both) of the following:\n",
    "1. Prediction\n",
    "2. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKctojwhfRnt"
   },
   "source": [
    "### Prediction\n",
    "\n",
    "Prediction means trying to estimate a result for the future based on the past. We humans predict something by acknowledging the data from the past and trying to guess or estimate the future. Similarly, machines can be taught using various learning techniques and they can then estimate or guess the future. Examples of prediction or domains where prediction can be applied are:\n",
    "\n",
    "- Weather forecasts\n",
    "- Disaster analysis\n",
    "- Stock markets\n",
    "- Traffic forecast\n",
    "\n",
    "For prediction, we have a set of _inputs_, $X$, readily available. The _output_, $Y$, is not available to us.\n",
    "\n",
    "We can then predict $Y$ as follows:\n",
    "$$\n",
    "\\hat{Y} = \\hat{f}(X)\n",
    "$$\n",
    "\n",
    "Where,\n",
    "- $\\hat{Y}$ is our prediction for $Y$, and,\n",
    "- $\\hat{f}(X)$ is our estimate for $f(X)$.\n",
    "\n",
    "Here, the error term, $\\varepsilon$, averages to zero.\n",
    "\n",
    "The accuracy of $\\hat{Y}$, as a prediction of $Y$, depends on 2 quantities,\n",
    "1. Reducible error\n",
    "2. Irreducible error\n",
    "\n",
    "In general, $\\hat{f}$ will not be an accurate estimate for $f$, and it will introduce a _reducible error_. We can reduce or minimize it using better statistical learning methods.\n",
    "\n",
    "Even though we perfectly estimate $f$ such that $\\hat{Y} = f(X)$, our prediction would still contain an error, because, $Y$ is also a function of $\\varepsilon$. Variability associated with $\\varepsilon$ also affects the accuracy of our prediction. This is the _irreducible error_. We cannot remove it how much ever we try.\n",
    "\n",
    "**Why is the irreducible error > 0?**\n",
    "\n",
    "$\\varepsilon$ may contain _unmeasurable variables_ that are useful in predicting $Y$. It may also contain _unmeasurable variations_.\n",
    "\n",
    "$$\n",
    "E(Y - \\hat{Y})^{2} = E[f(X) + \\varepsilon - \\hat{f}(X)]^{2} = [f(X) - \\hat{f}(X)]^{2} + var(\\varepsilon)\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $E(Y - \\hat{Y})^{2}$ is the _expected value_,\n",
    "- $[f(X) - \\hat{f}(X)]^{2}$ is the _squared difference_ between the predicted and actual value of $Y$. It is _reducible_ in nature.\n",
    "- $var({\\varepsilon})$ is the variance associated with $\\varepsilon$. It is irreducible in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SES4UO2Vx2oY"
   },
   "source": [
    "### Inference\n",
    "\n",
    "We are often interested in knowing how the output of a problem is affected by the input. We want to know how $Y$ is affected as $X = X_{1} + X_{2} + X_{3} + \\cdots + X_{p}$ changes.\n",
    "\n",
    "Thus, we want to understand the relationship between $X$ and $Y$.\n",
    "\n",
    "Here, $\\hat{f}$ cannot be treated as a \"_black box_\". We need the exact form of $\\hat{f}$.\n",
    "\n",
    "**We may want to infer,**\n",
    "- which predictors are _associated_ with the response,\n",
    "- what the _relationship between each_ predictor and the response is (positive, negative, etc.).\n",
    "- can the relationship between $Y$ and each predictor be adequately summarized using a linear equation or is the relationship more complicated (quadratic, cubic, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPHTH1Zv1P1I"
   },
   "source": [
    "### How do we estimate $f$?\n",
    "\n",
    "To estimate $f$, we need to teach our method. To teach, we have some data which we call as the _training data_. Training data is the dataset or part of the dataset which is used to train or teach the method on how to estimate $f$.\n",
    "\n",
    "**Characteristics of training data:**\n",
    "\n",
    "- $i$ denotes the $i^{th}$ observation out of the total $n$ observations.\n",
    "- $j$ denotes the $j^{th}$ predictor out of the $p$ total predictors.\n",
    "\n",
    "Thus, $x_{ij}$ is the $i^{th}$ observation of the $j^{th}$ predictor.\n",
    "\n",
    "Thus, $y_{i}$ is the response variable for the $i^{th}$ observation.\n",
    "\n",
    "Thus,\n",
    "\n",
    "Our training data set consists of,\n",
    "$$\n",
    "{(x_{1}, y_{1}), (x_{2}, y_{2}, \\cdots, (x_{n}, y_{n}))}\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "x_{i} = (x_{i1}, x_{i2}, \\cdots, x_{ip})^{T}\n",
    "$$\n",
    "\n",
    "Our goal is to apply a statistical learning method to our training data in order to estimate the unknown fucntion $f$.\n",
    "\n",
    "We want to find a function $f$ such that,\n",
    "$Y \\approx \\hat{f}(X)$, for any observation $(X, Y)$.\n",
    "\n",
    "Most statistical methods for this task can be classified into:\n",
    "\n",
    "1. Parametric methods\n",
    "2. Non-parametrix methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7oWjpFFT7JL"
   },
   "source": [
    "### Parametric Methods\n",
    "\n",
    "**Involves a 2-step, _model based_ approach.**\n",
    "\n",
    "**STEP 1:**\n",
    "\n",
    "We, first, make an assumption about the functional form, or shape, of $f$.\n",
    "\n",
    "One very simple, and maybe the first assumption we may make for any given problem, would be that $f$ is _linear_ in $X$:\n",
    "\n",
    "$$\n",
    "f(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{p}X_{p}\n",
    "$$\n",
    "\n",
    "This is a _linear model_.\n",
    "\n",
    "Linear models are very simple. Instead of having to estimate an entirely arbitrary $p$-dimensional function $f(X)$, one only needs to estimate the $p + 1$ coefficients, $\\beta_{0}, \\beta_{1}, \\beta_{2}, \\cdots, \\beta_{p}$.\n",
    "\n",
    "**STEP 2:**\n",
    "\n",
    "After a model is selected, a _procedure_ needs to be selected to _fit_ or _train_ the model. In case of our linear model, we need to estimate the parameters $\\beta_{0}, \\beta_{1}, \\beta_{2}, \\cdots, \\beta_{p}$ such that,\n",
    "\n",
    "$$\n",
    "Y \\approx \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\cdots + \\beta_{p}X_{p}\n",
    "$$\n",
    "\n",
    "The most common approach to fitting the model is referred to as \"**_(ordinary) least squares_**\". However, _least squares_ is one of the many possible methods to fit a linear model.\n",
    "\n",
    "**Thus, a _parametric method_ reduces the problem of estimating $f$ down to one of estimating a set of parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "\n",
    "A **_Decision Tree_** is a very basic model used in machine learning. It is not very accurate for predictions and much more powerful models do exist, but they are very easy to understand and sometimes also act as building blocks for the more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a simple decision tree:\n",
    "\n",
    "![Simple Decision Tree (PNG)](/res/simple_decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of recognizing patterns from data is called **fitting** or **training**. The data used to **train** or **fit** is called as **training data**.\n",
    "\n",
    "There can be more complex, and thus more accurate decision trees. An example of a more accurately predicting decision tree is given below:\n",
    "\n",
    "![Deeper Decision Tree (PNG)](/res/deeper_decision_tree.png)\n",
    "\n",
    "This tree gives more accurate predictions than say a tree like the one below:\n",
    "\n",
    "![Less Deep Decision Tree (PNG)](/res/less_deep_decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deeper decision trees have more '_splits_'. _Splits_ allow us to introduce more number of _factors_ in our decision making process. More the number of factors, better the prediction. The last node of the tree, where we obtain our prediction, is known as the **leaf** node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use some data to try out our new tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is an open-source Python data analysis library. We can use Pandas in our Python code by importing it. We generally import Pandas using the abbreviation **pd**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature of Pandas is its **_DataFrame_** object. A DataFrame is a container which holds the type of data which is similar to a table or an Excel sheet or a SQL table. This DataFrame object can then allow us to do a lot of things on the data using powerful methods in the Pandas library.\n",
    "\n",
    "As an example, we will be looking at data about home prices in Melbourne, Australia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Price</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13580.000000</td>\n",
       "      <td>1.358000e+04</td>\n",
       "      <td>13580.000000</td>\n",
       "      <td>13580.000000</td>\n",
       "      <td>13580.000000</td>\n",
       "      <td>13580.000000</td>\n",
       "      <td>13518.000000</td>\n",
       "      <td>13580.000000</td>\n",
       "      <td>7130.000000</td>\n",
       "      <td>8205.000000</td>\n",
       "      <td>13580.000000</td>\n",
       "      <td>13580.000000</td>\n",
       "      <td>13580.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.937997</td>\n",
       "      <td>1.075684e+06</td>\n",
       "      <td>10.137776</td>\n",
       "      <td>3105.301915</td>\n",
       "      <td>2.914728</td>\n",
       "      <td>1.534242</td>\n",
       "      <td>1.610075</td>\n",
       "      <td>558.416127</td>\n",
       "      <td>151.967650</td>\n",
       "      <td>1964.684217</td>\n",
       "      <td>-37.809203</td>\n",
       "      <td>144.995216</td>\n",
       "      <td>7454.417378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.955748</td>\n",
       "      <td>6.393107e+05</td>\n",
       "      <td>5.868725</td>\n",
       "      <td>90.676964</td>\n",
       "      <td>0.965921</td>\n",
       "      <td>0.691712</td>\n",
       "      <td>0.962634</td>\n",
       "      <td>3990.669241</td>\n",
       "      <td>541.014538</td>\n",
       "      <td>37.273762</td>\n",
       "      <td>0.079260</td>\n",
       "      <td>0.103916</td>\n",
       "      <td>4378.581772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.500000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1196.000000</td>\n",
       "      <td>-38.182550</td>\n",
       "      <td>144.431810</td>\n",
       "      <td>249.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.500000e+05</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>3044.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>1940.000000</td>\n",
       "      <td>-37.856822</td>\n",
       "      <td>144.929600</td>\n",
       "      <td>4380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.030000e+05</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>3084.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>-37.802355</td>\n",
       "      <td>145.000100</td>\n",
       "      <td>6555.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.330000e+06</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3148.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>651.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>-37.756400</td>\n",
       "      <td>145.058305</td>\n",
       "      <td>10331.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000e+06</td>\n",
       "      <td>48.100000</td>\n",
       "      <td>3977.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>433014.000000</td>\n",
       "      <td>44515.000000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>-37.408530</td>\n",
       "      <td>145.526350</td>\n",
       "      <td>21650.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Rooms         Price      Distance      Postcode      Bedroom2  \\\n",
       "count  13580.000000  1.358000e+04  13580.000000  13580.000000  13580.000000   \n",
       "mean       2.937997  1.075684e+06     10.137776   3105.301915      2.914728   \n",
       "std        0.955748  6.393107e+05      5.868725     90.676964      0.965921   \n",
       "min        1.000000  8.500000e+04      0.000000   3000.000000      0.000000   \n",
       "25%        2.000000  6.500000e+05      6.100000   3044.000000      2.000000   \n",
       "50%        3.000000  9.030000e+05      9.200000   3084.000000      3.000000   \n",
       "75%        3.000000  1.330000e+06     13.000000   3148.000000      3.000000   \n",
       "max       10.000000  9.000000e+06     48.100000   3977.000000     20.000000   \n",
       "\n",
       "           Bathroom           Car       Landsize  BuildingArea    YearBuilt  \\\n",
       "count  13580.000000  13518.000000   13580.000000   7130.000000  8205.000000   \n",
       "mean       1.534242      1.610075     558.416127    151.967650  1964.684217   \n",
       "std        0.691712      0.962634    3990.669241    541.014538    37.273762   \n",
       "min        0.000000      0.000000       0.000000      0.000000  1196.000000   \n",
       "25%        1.000000      1.000000     177.000000     93.000000  1940.000000   \n",
       "50%        1.000000      2.000000     440.000000    126.000000  1970.000000   \n",
       "75%        2.000000      2.000000     651.000000    174.000000  1999.000000   \n",
       "max        8.000000     10.000000  433014.000000  44515.000000  2018.000000   \n",
       "\n",
       "          Lattitude    Longtitude  Propertycount  \n",
       "count  13580.000000  13580.000000   13580.000000  \n",
       "mean     -37.809203    144.995216    7454.417378  \n",
       "std        0.079260      0.103916    4378.581772  \n",
       "min      -38.182550    144.431810     249.000000  \n",
       "25%      -37.856822    144.929600    4380.000000  \n",
       "50%      -37.802355    145.000100    6555.000000  \n",
       "75%      -37.756400    145.058305   10331.000000  \n",
       "max      -37.408530    145.526350   21650.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving filepath to a variable for easier access\n",
    "melb_data_filepath = 'melb_data.csv'\n",
    "# Read data from a CSV file and store it in a DataFrame object called melb_data\n",
    "melb_data = pd.read_csv(melb_data_filepath)\n",
    "# Print a summary of data in Melbourne data\n",
    "melb_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting data descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows 8 numbers for all the columns in the original dataset.\n",
    "\n",
    "The **count** column shows the number of rows containing non-missing values. Missing values are those for which there is no possible data. For example, the count for bedroom 2 in a 1 bedroom house will not be available and thus missing.\n",
    "\n",
    "The **mean** is an average of the values.\n",
    "\n",
    "**std** is for the standard deviation of the values. Standard deviation shows us how numerically the data is spread out. For more about standard deviation, refer to the basic statistics section.\n",
    "\n",
    "Sort the data in ascending order. The first value is the **min** value. **25%**, **50%** and **75%** are percentile values (_$x^{th}$ percentile_). They indicate the values which are bigger than $x$% of the values in the dataset and smaller than $(x - 100)$% of the values in the dataset. **max** is the largest number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Melbourne housing problem dataset has a lot of variables in it. It makes it difficult to grasp the data and understand it. We need to narrow down the number of columns (or factors) to those which actually matter. We will start by doing this intuitively.\n",
    "\n",
    "The **columns** property of the DataFrame object allows us to see a list of all the columns we have in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG',\n",
       "       'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',\n",
       "       'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',\n",
       "       'Longtitude', 'Regionname', 'Propertycount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will continue our Python code from the last cell containing Python code\n",
    "melb_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Melbourne dataset contains rows with missing values. We will learn how to handle missing values later, so for now we will just discard (or drop) those rows. To do so, we will use the **dropna** method. The **na** stands for **Not Available**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "melb_data = melb_data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the prediction target\n",
    "\n",
    "Using Pandas, one can pull out a single variable using the _dot-notation_. This single column is stored in a **_Series_**. It is similar to a DataFrame but with only a single column.\n",
    "\n",
    "We will use the _dot-notation_ to select a column which is called a **prediction target**. Our prediction target here would be the house prices. By convention, the prediction target is stored in a variable called ```y```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = melb_data.Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing features\n",
    "\n",
    "The columns in our dataset, which represent various variables, are known as _features_. Sometimes, we use all the columns (except our target) in our dataset for prediction and sometimes, we need to filter out some columns because they don't matter much or are not useful for our prediction.\n",
    "\n",
    "We can select multiple columns by providing our algorithm with an array of all the columns which we want. Each column name should be of _string_ datatype. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "melb_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, we denote this as ```X```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = melb_data[melb_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the summary for ```X```. After that, we shall have a look at the first few rows of the data using the **head** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6196.000000</td>\n",
       "      <td>6196.000000</td>\n",
       "      <td>6196.000000</td>\n",
       "      <td>6196.000000</td>\n",
       "      <td>6196.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.931407</td>\n",
       "      <td>1.576340</td>\n",
       "      <td>471.006940</td>\n",
       "      <td>-37.807904</td>\n",
       "      <td>144.990201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.971079</td>\n",
       "      <td>0.711362</td>\n",
       "      <td>897.449881</td>\n",
       "      <td>0.075850</td>\n",
       "      <td>0.099165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-38.164920</td>\n",
       "      <td>144.542370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>-37.855438</td>\n",
       "      <td>144.926198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>-37.802250</td>\n",
       "      <td>144.995800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>628.000000</td>\n",
       "      <td>-37.758200</td>\n",
       "      <td>145.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>37000.000000</td>\n",
       "      <td>-37.457090</td>\n",
       "      <td>145.526350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Rooms     Bathroom      Landsize    Lattitude   Longtitude\n",
       "count  6196.000000  6196.000000   6196.000000  6196.000000  6196.000000\n",
       "mean      2.931407     1.576340    471.006940   -37.807904   144.990201\n",
       "std       0.971079     0.711362    897.449881     0.075850     0.099165\n",
       "min       1.000000     1.000000      0.000000   -38.164920   144.542370\n",
       "25%       2.000000     1.000000    152.000000   -37.855438   144.926198\n",
       "50%       3.000000     1.000000    373.000000   -37.802250   144.995800\n",
       "75%       4.000000     2.000000    628.000000   -37.758200   145.052700\n",
       "max       8.000000     8.000000  37000.000000   -37.457090   145.526350"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>-37.8079</td>\n",
       "      <td>144.9934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>-37.8093</td>\n",
       "      <td>144.9944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-37.8072</td>\n",
       "      <td>144.9941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>-37.8024</td>\n",
       "      <td>144.9993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>-37.8060</td>\n",
       "      <td>144.9954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rooms  Bathroom  Landsize  Lattitude  Longtitude\n",
       "1      2       1.0     156.0   -37.8079    144.9934\n",
       "2      3       2.0     134.0   -37.8093    144.9944\n",
       "4      4       1.0     120.0   -37.8072    144.9941\n",
       "6      3       2.0     245.0   -37.8024    144.9993\n",
       "7      2       1.0     256.0   -37.8060    144.9954"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This check on data using visual methods is a very important job that every data scientist should be skilled at. This is known as **data visualization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **scikit-learn** to build our model. It is a very popular open-source library for building models using the type of data which is stored using the DataFrame object. It is denoted by **sklearn** in Python code.\n",
    "\n",
    "The following are the steps followed while building and running a mdoel:\n",
    "\n",
    "* **Define**: What type of a model will it be? Will it be a decision tree? Or something else? Other parameters related to the type of model are also specified at this step.\n",
    "* **Fit/Train**: Capture patterns in the provided data (training data). This is the most important step.\n",
    "* **Predict**: Self-explanatory.\n",
    "* **Evaluate**: Determine the accuracy of our model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a decision tree model using **scikit-learn** and fitting it with features and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Define a model and set random_state to 1 to ensure same results after each run.\n",
    "melb_model = DecisionTreeRegressor(random_state=1)\n",
    "# Fit the model.\n",
    "melb_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning models allow some randomness in their model training. When we specify a number for ```random_state```, we are ensuring that we get the same result in each run. This is considered as good practice. The model quality won't depend on the value of ```random_state``` that you choose.\n",
    "\n",
    "We now have fitted a model which we can use to predict value. In practice, we will make predictions for the newer houses being built, but, for now, we will make predictions for the first few rows of the dataset just to see how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for the following 5 houses:\n",
      "   Rooms  Bathroom  Landsize  Lattitude  Longtitude\n",
      "1      2       1.0     156.0   -37.8079    144.9934\n",
      "2      3       2.0     134.0   -37.8093    144.9944\n",
      "4      4       1.0     120.0   -37.8072    144.9941\n",
      "6      3       2.0     245.0   -37.8024    144.9993\n",
      "7      2       1.0     256.0   -37.8060    144.9954\n",
      "The predictions are:\n",
      "[1035000. 1465000. 1600000. 1876000. 1636000.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Making predictions for the following 5 houses:\");\n",
    "print(X.head())\n",
    "print(\"The predictions are:\")\n",
    "print(melb_model.predict(X.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built a decision tree model, it is natural to ask how accurate our model's predictions are and how to improve them. We can do that with model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model validation is used to measure the **quality of the model**. The quality of the model is the key to iteratively improve upon the model. We will likely want to assess the quality of every model we ever build, and in most cases, the _predictive accuracy_ is a very relevant measure of model quality. In easier words, we want to know how close the predictions given by a model are to what actually happens.\n",
    "\n",
    "A lot of people make a big mistake when validating a model. They use the _training_ data to make predictions and compare the predicted results with the target variable in the _training_ data. We shall see the problems which occur when using the _training_ data for predictions later on. One should insteead use the _testing_ data for making the predictions.\n",
    "\n",
    "Our data may have tens of thousands of rows of data. To compare the predicted and actual values to asses the model quality by labelling each row's prediction to be either good or bad would take a lot of manpower and time. Instead, we need to summarize the whole set of data and the predictions into one single metric. There are many metrics for summarizing the model quality, but we shall have a look at a basic one called **Mean Absolute Error (MAE)**.\n",
    "\n",
    "The prediction error for each row (each house here) of data would be\n",
    "\n",
    "```\n",
    "error = actual - predicted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if a house costs \\\\$100,000 and the predicted price was \\\\$80,000, then the error is \\\\$20,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the absolute value of errors in the **MAE** metric. Then we take the average of these absolute errors. This average is our **MAE** for that data. In short, **Mean Absolute Error** means, \"on average, our predictions are off by about X\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the MAE for our Melbourne model (```melb_model```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115.7467183128902"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "predicted_home_prices = melb_model.predict(X)\n",
    "mean_absolute_error(y, predicted_home_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we obtained is known as an \"**In-sample**\" score. An in-sample score is the one which is obtained where the data or the \"**sample**\" used for predictions is the same as the one used to train or build the model.\n",
    "\n",
    "What our model does is recognize patterns in the data. Now, when we predict the model using the same data, it will definitely recognize the patterns and will predict accurately. But what if we introduce some data to predict which is not present in the original dataset (or the dataset used to train the model)? The model should still predict with the same accuracy. If it doesn't, it indicates a low quality of our model.\n",
    "\n",
    "In real life, we don't make predictions for the data for which we already have the answers, but we use unknown data which the model might not have ever seen before and we predict results for that data. If the results for unknown data are accurate, it indicates a high quality of our model. Our model is performing well then.\n",
    "\n",
    "We exclude some data from our original dataset while building our model and use that excluded data which the model has never seen before to test our model's performance. This excluded data is known as **validation data** or **testing data**. The data used for building or training the model is known as **training data**.\n",
    "\n",
    "**scikit-learn** provides us with a method to split the data into _training_ and _testing_. It is called ```train_test_split```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy\n",
    "\n",
    "#### Basics\n",
    "\n",
    "NumPy is a Python library which provides us with support for large multi-dimensional arrays and matrices. It also provides high-level mathematical functions to operate on these arrays.\n",
    "\n",
    "NumPy's main object is a _homogeneous multi-dimensional_ array. It is a table of elements which are mostly numeric. All elements in an array are of the same type. They are indexed by a tuple of positive integers.\n",
    "\n",
    "In NumPy, _dimensions_ are called **axes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example,\n",
    "\n",
    "```\n",
    "[1, 3, 8]\n",
    "```\n",
    "The above given array has 1 axis and that axis contains 2 elements. Thus, the length of that axis is 2.\n",
    "\n",
    "```\n",
    "[[3, 1, 6],\n",
    "[5, 5, 0]]\n",
    "```\n",
    "The above given array has 2 axes. The first axis contains 2 elements (```[3, 1, 6]``` and ```[5, 5, 0]```). The second axis contains 3 elements (```3```, ```1``` and ```6```)(```5```, ```5``` and ```0```)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy's array class is known as ```ndarray``` which stands for **n-dimensional array**. ```ndarray``` is aliased by ```array```. The standard Python array class, ```array.array``` is not the same as NumPy's array class, ```numpy.array```. ```array.array``` offer way less functionality and handles only 1-dimensional arrays.\n",
    "\n",
    "Following are the important attributes of the ```ndarray``` class:\n",
    "\n",
    "* ```ndarray.ndim```\n",
    "\n",
    "    The number of axes (dimensions) of the array.\n",
    "    \n",
    "* ```ndarray.shape```\n",
    "\n",
    "    This is a tuple of integers which indicate the size of the array in each dimension. For example, for a matrix with _n_ rows and _m_ columns, the ```shape``` would be ```(n, m)```. The length of the ```shape``` tuple is therefore the number of axes, ```ndim```.\n",
    "    \n",
    "* ```ndarray.size```\n",
    "\n",
    "    This gives us the total number of elements of an array. It is equal to the product of the elements of ```shape```.\n",
    "\n",
    "* ```ndarray.dtype```\n",
    "\n",
    "    This is an object describing the type of the elements in the array. ```dtype```s can be specified using standard Python types. Additionally, NumPy provides with its own types like ```numpy.int32```, ```numpy.int16```, ```numpy.float64```.\n",
    "\n",
    "* ```ndarray.itemsize```\n",
    "\n",
    "    This gives us the size in bytes of each element of the array. For example, for an array of elements of type ```float64```, the size would be 8 (8 bytes = 64 bits), for an array of elements of type ```complex32```, the size would be 4 (4 bytes = 32 bits). ```ndarray.itemsize``` is equivalent to ```ndarray.dtype.itemsize```.\n",
    "\n",
    "* ```ndarray.data```\n",
    "\n",
    "    This is the buffer containing the actual elements of an array. Normally, we won't require this as we will access elements using indexing facilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out how to use NumPy in our code. We use NumPy in our Python code as ```np```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the above given attributes with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array0 = np.arange(15)\n",
    "array0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```arange``` method allows us to create an array with integer elements from ```0``` to the value passed to the ```arange``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [ 5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array0 = array0.reshape(3, 5)\n",
    "array0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```reshape``` method allows us to change the shape of our array. The ```3, 5``` mean that the array will display as a matrix of 3 rows with 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array1 = np.arange(18).reshape(6, 3)\n",
    "array1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array0.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2],\n",
       "        [2, 3]],\n",
       "\n",
       "       [[3, 4],\n",
       "        [4, 5]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = np.array([[[1, 2], [2, 3]], [[3, 4], [4, 5]]])\n",
    "array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array0.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "array0_dtype = array0.dtype.name\n",
    "array1_dtype = array1.dtype.name\n",
    "array2_dtype = array2.dtype.name\n",
    "print(array0_dtype)\n",
    "print(array1_dtype)\n",
    "print(array2_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```name``` feature of any ```dtype``` object extracts the name of the type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "itemsize_array0 = array0.itemsize\n",
    "itemsize_array1 = array1.itemsize\n",
    "itemsize_array2 = array2.itemsize\n",
    "print(itemsize_array0)\n",
    "print(itemsize_array1)\n",
    "print(itemsize_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "18\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "size_array0 = array0.size\n",
    "size_array1 = array1.size\n",
    "size_array2 = array2.size\n",
    "print(size_array0)\n",
    "print(size_array1)\n",
    "print(size_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(array0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array creation\n",
    "\n",
    "There are several ways of creating arrays using NumPy.\n",
    "\n",
    "One can create arrays using a regular Python list or tuple using the ```array``` function. The type of the array obtained is automatically determined from the type of elements in the sequences."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
